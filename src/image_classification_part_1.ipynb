{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Image Classification\n",
    "\n",
    "**Responsible Person: Akshi**\n",
    "\n",
    "**Estimated Duration: 15 mins**\n",
    "\n",
    "### What is Image Classification?\n",
    "\n",
    "Image classification is the process of determining what is shown in an image.\n",
    "\n",
    "[Silicon Valley - Hot Dog Not Hot Dog](https://www.youtube.com/watch?v=ACmydtFDTGs)\n",
    "\n",
    "We can use deep learning to do this for us. When classifying images using deep learning, we use a convolutional neural network (CNN). CNNs are specifically designed to process images. For this session, we will steer clear of the theory behind CNN's and focus on the practical stuff.\n",
    "\n",
    "![https://www.google.ca/url?sa=i&rct=j&q=&esrc=s&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwi5gPbtgofdAhUVIDQIHaWvCvgQjRx6BAgBEAU&url=https%3A%2F%2Fgithub.com%2Ftavgreen%2Flanduse_classification&psig=AOvVaw0a9VXQ-t1Hm1QcJnpfawaa&ust=1535245687702429](../additional/img/number_3_cnn.png)\n",
    "\n",
    "## How do CNNs learn to classify?\n",
    "\n",
    "First we need to decide what we want to teach our model.\n",
    "\n",
    "Do we want our model to correctly identify:\n",
    "\n",
    "* Cats and dogs?\n",
    "\n",
    "* Different types of cats?\n",
    "\n",
    "* Different types of dogs?\n",
    "\n",
    "* Different types of flowers?\n",
    "\n",
    "* Everything?\n",
    "\n",
    "CNNs work in a similar way as a human brain (inspired by the way the visual cortex works). If we, as humans, are exposed to something new, it takes time for us to learn what it is.\n",
    "\n",
    "### Can you identify this berry?\n",
    "\n",
    "![](../additional/img/Wild_red_baneberry_1.jpg)\n",
    "\n",
    "If our brain hasn't been exposed to to something, classification becomes a guessing game. This applies to deep learning as well.\n",
    "\n",
    "We need to teach our model what different berries look like.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We need to train our model what the difference is between the different classes.\n",
    "\n",
    "After training, when the model is faced with a new image that it hasn't seen before, it needs to decide for itself what is most likely shown in the image.\n",
    "\n",
    "![](../additional/img/cat_dog.png)\n",
    "\n",
    "![](../additional/img/cat_dog_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training / Retraining\n",
    "\n",
    "**Responsible Person: Xinbin**\n",
    "\n",
    "**Estimated Duration: 15 mins**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained architechture & Transfer Learning\n",
    "\n",
    "**Architechture used**: [MobileNet](https://ai.googleblog.com/2017/06/mobilenets-open-source-models-for.html) is a a small efficient convolutional neural network, which is designed to accomodate the restricted resources for an on-device or embedded application.\n",
    "\n",
    "The MobileNet is configurable in two ways:\n",
    "\n",
    "- Input image resolution: 128,160,192, or 224px. Unsurprisingly, feeding in a higher resolution image takes more processing time, but results in better classification accuracy.\n",
    "- The relative size of the model as a fraction of the largest MobileNet: 1.0, 0.75, 0.50, or 0.25.\n",
    "\n",
    "We will use 224 and 0.5 for the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobilenet_0.5_224\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "IMAGE_SIZE=224\n",
    "MODEL_SIZE=0.5\n",
    "ARCHITECTURE=\"mobilenet_${REL_SIZE}_${IMAGE_SIZE}\"\n",
    "\n",
    "# print the architure name\n",
    "echo $ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining script\n",
    "The retrain script is from the [TensorFlow Hub repo](https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py), and we have included in the workshop repo.\n",
    "\n",
    "Before running the script, there are a few arguments worth mentioning:\n",
    "\n",
    "- **bottleneck_dir** : path to cache bottleneck layer values as files\n",
    "- **how_many_training_steps** : How many training steps to run before ending\n",
    "- **model_dir** : path to save the trained model information, e.g. graph, parameters, and etc.\n",
    "- **summaries_dir** : Where to save summary logs for TensorBoard\n",
    "- **output_graph** : Where to save the trained graph\n",
    "- **output_labels** : path to save the trained graph's labels\n",
    "- **architecture** : the model architecture to use  \n",
    "- **image_dir** : path to labeled images for training\n",
    "\n",
    "You can retrive the whole list of arguments using the following command.\n",
    "\n",
    "```bash\n",
    "python -m scripts.retrain -h\n",
    "```\n",
    "\n",
    "Let's run the training with the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m scripts.retrain \\\n",
    "  --bottleneck_dir=tf_files/bottlenecks \\\n",
    "  --how_many_training_steps=500 \\\n",
    "  --model_dir=tf_files/models/ \\\n",
    "  --summaries_dir=tf_files/training_summaries/\"${ARCHITECTURE}\" \\\n",
    "  --output_graph=tf_files/retrained_graph.pb \\\n",
    "  --output_labels=tf_files/retrained_labels.txt \\\n",
    "  --architecture=\"${ARCHITECTURE}\" \\\n",
    "  --image_dir=tf_files/data/train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work? \n",
    "The above script downloads the pre-trained model, adds a new final layer, and trains that layer on the cat/dog photos we provided. It contains two main phases:\n",
    "1. Calculates and caches the bottleneck values for each image\n",
    "2. Actual training of the final layer which makes the classification\n",
    "\n",
    "The techinques that make the training possible is **Transfer Learning**.\n",
    "\n",
    "### Transfer Learning\n",
    "**Transfer learning** is a machine learning method where a model developed for a related task is reused as the starting point for a new model. It has the following benefits\n",
    "\n",
    "- Utilize the power of pre-trained model to extract features from images\n",
    "- Faster...\n",
    "- Less data & less resource (Google: 1000x computing power to replace ml expert)\n",
    "\n",
    "The image below summarize the process. (Image retrived from a talk at [Google Cloud Next '17](https://www.youtube.com/watch?v=EnFyneRScQ8&feature=youtu.be&t=4m17s) by *Yufeng Guo*)\n",
    "![](../additional/img/retrain.png)\n",
    "\n",
    "### Bottlenecks \n",
    "A **bottleneck** is an informal term we often use for the layer just before the final output layer that actually does the classification (TensorFlow Hub calls this an \"image feature vector\"). This penultimate layer has been trained to output a set of values that's good enough for the classifier to use to distinguish between all the classes it's been asked to recognize.\n",
    "\n",
    "Because every image is reused multiple times during training and calculating each bottleneck takes a significant amount of time, it speeds things up to cache these bottleneck values on disk so they don't have to be repeatedly recalculated. The command you ran saves these files to the `bottlenecks/` directory. If you rerun the script, they'll be reused, so you don't have to wait for this part again.\n",
    "\n",
    "### Actual training\n",
    "You'll see a series of step outputs, each one showing training accuracy, validation accuracy, and the cross entropy. \n",
    "- **training accuracy** : percent of the images used in the current training batch were labeled with the correct class. \n",
    "- **validation accuracy** : the precision on a randomly-selected group of images different from the training.\n",
    "    - **Overfitting** : model may overfit to the noise during training, so we use **validation accuracy** to measure the true performance. If the train accuracy is high but the validation accuracy remains low, that means the network is overfitting and remembering noise\n",
    "- **cross entropy** : a loss function which gives a glimpse into how well the learning process is progressing. It should keep going down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "**Responsible Person: Johannes**\n",
    "\n",
    "**Estimated Duration: 10 mins**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# python scripts/retrain.py --image_dir ../tf_files/data/train \\\n",
    "#     --tfhub_module https://tfhub.dev/google/imagenet/mobilenet_v1_075_160/feature_vector/1 \\\n",
    "#     --how_many_training_steps 1000 \\\n",
    "#     --train_batch_size 25 \\\n",
    "#     --summaries_dir tmp/retrain_logs \\\n",
    "#     --output_graph tmp/output_graph.pb \\\n",
    "#     --output_labels tmp/output_labels.txt\n",
    "\n",
    "# %%capture capt\n",
    "\n",
    "# %run -i scripts/retrain.py --image_dir ../additional/data/train \\\n",
    "#     --tfhub_module https://tfhub.dev/google/imagenet/mobilenet_v1_075_160/feature_vector/1 \\\n",
    "#     --how_many_training_steps 1000 \\\n",
    "#     --train_batch_size 25 \\\n",
    "#     --summaries_dir tmp/retrain_logs \\\n",
    "#     --output_graph tmp/output_graph.pb \\\n",
    "#     --output_labels tmp/output_labels.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is TensorBoard?\n",
    "\n",
    "TensorBoard is a suite of visualization tools. The goal of Tensorboard is to remove some of the complexity and confusion behind deep learning. TensorBoard can be used to:\n",
    "\n",
    "* visualize your Tensorflow graph\n",
    "* plot quantitative metrics about training and validation of your model\n",
    "\n",
    "## How do I access TensorBoard?\n",
    "\n",
    "Run the following following command in bash:\n",
    "\n",
    "`tensorboard --logdir tmp/retrain_logs`\n",
    "\n",
    "or alternatively run the code chunk below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jharmse/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "2018-09-10 16:08:35.654920: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-09-10 16:08:35.748898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-09-10 16:08:35.749550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \n",
      "name: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493\n",
      "pciBusID: 0000:01:00.0\n",
      "totalMemory: 3.95GiB freeMemory: 3.30GiB\n",
      "2018-09-10 16:08:35.749582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\n",
      "2018-09-10 16:08:36.074260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2018-09-10 16:08:36.074289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \n",
      "2018-09-10 16:08:36.074296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \n",
      "TensorBoard 1.8.0 at http://jharmse:6006 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "! tensorboard --logdir tmp/retrain_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the last line. It should say something like:\n",
    "\n",
    "`TensorBoard 1.8.0 at http://jharmse:6006 (Press CTRL+C to quit)`\n",
    "\n",
    "This means that TensorBoard is available at `http://jharmse:6006`\n",
    "\n",
    "Type in your equivalent of `jharmse:6006` into your browser (Chrome, Firefox, etc.)\n",
    "\n",
    "You should see TensorBoard.\n",
    "\n",
    "**Remember: Once you are done exploring Tensorboard, go to the place where you launched TensorBoard and press `CTRL+C` to quit TensorBoard. Otherwise it will keep on running in the background.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few interesting Tensorflow features\n",
    "\n",
    "### Scalars\n",
    "\n",
    "Here you can visualize any recording you decided to make during model training. Things you might be interested in visualizing are things like: \n",
    "    * model accuracy across iterations\n",
    "    * the cross entropy (certainty of model predictions) across iterations.\n",
    "    \n",
    "You can visualizations for different runs, like training and validation. This can help you gain a deeper understanding of the model's performance. For example, if a model's training accuracy is very high towards the end of the iterations, but the validation accuracy is low, it means that the model has started memorizing the training data instead of simply learning the features of the images.\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog 0.9991627\n",
      "cat 0.0008373484\n"
     ]
    }
   ],
   "source": [
    "%run -i scripts/label_image.py --image ../tf_files/data/test/4.jpg \\\n",
    "    --graph tmp/output_graph.pb \\\n",
    "    --labels tmp/output_labels.txt \\\n",
    "    --input_height 160 \\\n",
    "    --input_width 160 \\\n",
    "    --input_layer Placeholder \\\n",
    "    --output_layer final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions using Trained Model\n",
    "\n",
    "**Responsible Person: Akshi**\n",
    "\n",
    "**Estimated Duration: 15 mins**\n",
    "\n",
    "#### Notes\n",
    "\n",
    "* Use ~3 image examples (2 good, 1 ambiguous)\n",
    "\n",
    "* Talk about interpreting the class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning (Optional)\n",
    "\n",
    "**Responsible Person: Xinbin**\n",
    "\n",
    "**Estimated Duration: TBD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Different Image Dataset (Optional)\n",
    "\n",
    "**Responsible Person: Akshi**\n",
    "\n",
    "**Estimated Duration: TBD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**Responsible Person: Johannes**\n",
    "\n",
    "**Estimated Duration: 15 mins**\n",
    "\n",
    "#### Notes\n",
    "\n",
    "* Talk about coding challenge\n",
    "\n",
    "* Give a taster for the theory to be covered in the next meetup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
